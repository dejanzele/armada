cyclePeriod: 1s
schedulePeriod: 10s
maxSchedulingDuration: 5s
executorTimeout: 1h
databaseFetchSize: 1000
pulsarSendTimeout: 5s
internedStringsCacheSize: 100000
queueRefreshPeriod: 10s
publishMetricsToPulsar: false
metrics:
  port: 9000
  jobStateMetricsResetInterval: 12h
  refreshInterval: 30s
  trackedResourceNames:
    - "cpu"
    - "memory"
    - "ephemeral-storage"
    - "nvidia.com/gpu"
pulsar:
  URL: "pulsar://pulsar:6650"
  jobsetEventsTopic: "events"
  metricEventsTopic: "metrics"
  maxConnectionsPerBroker: 1
  compressionType: zlib
  compressionLevel: faster
  maxAllowedEventsPerMessage: 1000
  maxAllowedMessageSize: 4194304 #4Mi
  sendTimeout: 5s
armadaApi:
  armadaUrl: "server:50051"
  forceNoTls: true
priorityMultiplier:
  enabled: false
priorityOverride:
  enabled: false
pricingApi:
  enabled: false
postgres:
  connection:
    host: postgres
    port: 5432
    user: postgres
    password: psw
    dbname: scheduler
    sslmode: disable
leader:
  mode: standalone
  leaseLockName: armada-scheduler
  LeaseLockNamespace: "" # This must be set so viper allows env vars to overwrite it
  leaseDuration: 15s
  renewDeadline: 10s
  retryPeriod: 2s
  podName: "" # This must be set so viper allows env vars to overwrite it
  leaderConnection:
    armadaUrl: "" # <name> will get replaced with the lease owners name
http:
  port: 8080
grpc:
  port: 50052
  keepaliveParams:
    maxConnectionIdle: 5m
    time: 120s
    timeout: 20s
  keepaliveEnforcementPolicy:
    minTime: 10s
    permitWithoutStream: true
  tls:
    enabled: false
# You may want to configure indexedNodeLabels and indexedTaints to speed up scheduling.
scheduling:
  pools:
    - name: default
  supportedResourceTypes:
    - name: memory
      resolution: "1"
    - name: cpu
      resolution: "1m"
    - name: ephemeral-storage
      resolution: "1"
    - name: nvidia.com/gpu
      resolution: "1"
  disableScheduling: false
  disableIndependentPoolFailures: true
  enableAssertions: false
  enablePreferLargeJobOrdering: false
  protectedFractionOfFairShare: 1.0
  nodeIdLabel: "kubernetes.io/hostname"
  priorityClasses:
    armada-default:
      priority: 1000
      preemptible: false
      maximumResourceFractionPerQueue:
        memory: 1.0
        cpu: 1.0
    armada-preemptible:
      priority: 1000
      preemptible: true
  defaultPriorityClassName: "armada-default"
  priorityClassNameOverride: "armada-default"
  maxQueueLookback: 100000
  maximumResourceFractionToSchedule:
    memory: 1.0
    cpu: 1.0
  maximumSchedulingRate: 100.0
  maximumSchedulingBurst: 1000
  maximumPerQueueSchedulingRate: 50.0
  maximumPerQueueSchedulingBurst: 1000
  maxJobSchedulingContextsPerExecutor: 10000
  maxRetries: 3
  # Retry policy provides fine-grained control over job retry behavior.
  # When enabled, it takes precedence over maxRetries.
  # Default behavior: all failures fail immediately (no retry).
  # Add Retry rules to opt-in to retry behavior for specific conditions.
  retryPolicy:
    enabled: false  # Set to true to enable retry policy
    globalMaxRetries: 20  # Hard cap on total retries across all policies
    default:
      retryLimit: 3
      rules: []  # No rules = all failures fail immediately
    # Named policies can be referenced by queue's retry_policy field
    # policies:
    #   ml-training:
    #     retryLimit: 10
    #     rules:
    #       - action: Retry
    #         onExitCodes:
    #           operator: In
    #           values: [137, 143]
  dominantResourceFairnessResourcesToConsider:
    - "cpu"
    - "memory"
    - "nvidia.com/gpu"
    - "ephemeral-storage"
  indexedResources:
    - name: "nvidia.com/gpu"
      resolution: "1"
    - name: "cpu"
      resolution: "100m"
    - name: "memory"
      resolution: "100Mi"
    - name: "ephemeral-storage"
      resolution: "1Gi"
  executorTimeout: "10m"
  maxUnacknowledgedJobsPerExecutor: 2500
  executorUpdateFrequency: "60s"
  preemptionRetry:
    enabled: true
  experimentalIndicativePricing:
    basePrice: 100.0
    basePriority: 500.0
  experimentalIndicativeShare:
    basePriorities: []
